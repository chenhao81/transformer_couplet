{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer 的学习笔记，尝试作对联\n",
    "\n",
    "本文是根据官方transformer的例子改写的，官方的例子是做了个葡萄牙语和英语的翻译，我这里是用对联数据集训练的transformer模型，给出第一句，作出后一句。主要的改动其实在数据准备，模型都没有啥变化。\n",
    "\n",
    "官方笔记如下：\n",
    "\n",
    "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61aQu43dvfdy"
   },
   "source": [
    "## 1.1 环境准备\n",
    "\n",
    "和 RNN学习笔记一样，先把环境准备好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beWM0Zt6-tJr"
   },
   "source": [
    "安装基础库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzKI4E6O-uxi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mitdeeplearning in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from mitdeeplearning) (2019.8.19)\n",
      "Requirement already satisfied: gym in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from mitdeeplearning) (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from mitdeeplearning) (1.17.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from mitdeeplearning) (4.36.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym->mitdeeplearning) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym->mitdeeplearning) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym->mitdeeplearning) (1.4.1)\n",
      "Requirement already satisfied: future in c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.18.2)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Tensorflow 2.0\n",
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf \n",
    "\n",
    "# Download and import the MIT 6.S191 package\n",
    "!pip install mitdeeplearning\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "# Import all remaining packages\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppWeQAa-vsw7"
   },
   "source": [
    "## 1.2 预处理数据\n",
    "准备数据，去掉空行、回车之类的符号:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlH2fN0s3XgW"
   },
   "source": [
    "尝试从处理完的文件中，读取一些片段看看是否满意,现在每一行应该是一整首诗："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfSEOlSI2rYx"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/mnt')\n",
    "\n",
    "# path = \"/mnt/My Drive/couplet\"\n",
    "# os.chdir(path)\n",
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPMVkoK452Qi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取完毕，总共770491行\n",
      "处理完毕，总共770487行\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open(\"./couplet/train/in.txt\",\"rt\",encoding='UTF-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "print(\"读取完毕，总共%d行\"%(len(lines)))\n",
    "\n",
    "\n",
    "inp_lines = []\n",
    "\n",
    "# 删除过长行\n",
    "for line in lines:\n",
    "  if len(line)< 65:\n",
    "    inp_lines += [line.replace(\" \",\"\")]\n",
    "\n",
    "print(\"处理完毕，总共%d行\"%(len(inp_lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGOPG1EV6dqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取完毕，总共770491行\n",
      "处理完毕，总共770487行\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./couplet/train/out.txt\",\"rt\",encoding='UTF-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "print(\"读取完毕，总共%d行\"%(len(lines)))\n",
    "\n",
    "\n",
    "output_lines = []\n",
    "\n",
    "# 删除过长行\n",
    "for line in lines:\n",
    "  if len(line)< 65:\n",
    "    output_lines += [line.replace(\" \",\"\")]\n",
    "\n",
    "print(\"处理完毕，总共%d行\"%(len(output_lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbvqBlPf71Vj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全文共使用了 9121 个不同的文字\n",
      "0 : ,\n",
      "vectorized_lines [rank:tf.Tensor(1, shape=(), dtype=int32) shape:tf.Tensor([14411484], shape=(1,), dtype=int32)]\n",
      "'处毋人负我，毋我负人' ---- characters mapped to int ----> [1420, 3672, 129, 7316, 2540, 9117, 3672, 2540, 7316, 129]\n"
     ]
    }
   ],
   "source": [
    "lines_joined = \"\\n\\n\".join(inp_lines + output_lines) \n",
    "\n",
    "lines_joined = lines_joined.replace(\"\\n\",\"\")\n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "vocab = sorted(set(lines_joined))\n",
    "print(\"全文共使用了\", len(vocab), \"个不同的文字\")\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print(\"0 : %s\"%idx2char[0])\n",
    "\n",
    "### 将诗歌向量化  ###\n",
    "def vectorize_string(string):\n",
    "  return list(map(lambda x:char2idx[x],string))\n",
    "\n",
    "vectorized_lines = vectorize_string(lines_joined)\n",
    "\n",
    "print('vectorized_lines [rank:%s shape:%s]'%(tf.rank(vectorized_lines),tf.shape(vectorized_lines)))\n",
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(lines_joined[50:60]), vectorized_lines[50:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpAIKILNqhSk"
   },
   "source": [
    "因为transformer需要成对出现的训练数据，我们这里把诗的第一句提取出来作为训练数据的输入，把诗的后面几句作为训练数据的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDLLCVQu3wbn"
   },
   "source": [
    "构建数据集dataset，每一个输入、输出的前后都加上开始、结束标记（vocab_size，vocab_size+1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1a1AeKcuQ0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 770487)\n",
      "本领高强攀月桂\n",
      "\n",
      "成亲吉利放兰香\n",
      "\n",
      "[9121, 3191, 8489, 8716, 2194, 2915, 3170, 3351, 9122]\n",
      "[9121, 2539, 122, 818, 565, 2930, 456, 8614, 9122]\n",
      "豪华超御苑\n",
      "\n",
      "康乐驻山城\n",
      "\n",
      "[9121, 7283, 705, 7399, 2252, 6297, 9122]\n",
      "[9121, 2126, 67, 8665, 1834, 1305, 9122]\n",
      "常夸海口无人信\n",
      "\n",
      "不做夜郎有客邀\n",
      "\n",
      "[9121, 2067, 1452, 3931, 789, 3016, 129, 306, 9122]\n",
      "[9121, 19, 362, 1436, 7780, 3171, 1713, 7734, 9122]\n"
     ]
    }
   ],
   "source": [
    "# 数据集\n",
    "vocab_size = len(vocab)\n",
    "dataset = [[],[]]   # input_lines, output_lines\n",
    "for i in range(0,len(inp_lines)):\n",
    "  dataset[0] += [[vocab_size] + vectorize_string(inp_lines[i].replace(\"\\n\",\"\")) + [(vocab_size + 1)]]\n",
    "  dataset[1] += [[vocab_size] + vectorize_string(output_lines[i].replace(\"\\n\",\"\")) + [(vocab_size + 1)]]\n",
    "\n",
    "dataset = np.asarray(dataset)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "for i in range(10,13):\n",
    "  print(inp_lines[i])\n",
    "  print(output_lines[i])\n",
    "  print(dataset[0][i])\n",
    "  print(dataset[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4dIULHvtaFD"
   },
   "source": [
    "随机取出batch_size个输入输出序列数据，保证每一批数据里所有输入序列长度都是一致的，所有输出序列的长度也是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnG_2DIctZSq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9121 8297 6895 1420   94 8614 3752 7090 9122    0    0    0    0    0] -> [9121 1798 2034 7680 1003 3848 7908 6387 9122    0    0    0    0    0]\n",
      "[9121 7145  774  713 1834 9117 2267 7680 5233  189    8 3229 7838 9122] -> [9121 8265 7068   33 7715 9117 3170 4898 8527 4008 2953  791 7167 9122]\n"
     ]
    }
   ],
   "source": [
    "### 从 vectorized_lines 中随机选 batch_size 个序列\n",
    "### 输出 input,target 序列数组\n",
    "def get_batch(dataset, batch_size):\n",
    "  # the length of the vectorized lines string\n",
    "  n = dataset.shape[1]\n",
    "\n",
    "  # randomly choose the starting indices for the examples in the training batch\n",
    "  idx = np.random.choice(n, batch_size)\n",
    "\n",
    "  '''construct a list of input sequences for the training batch'''\n",
    "  max_inp_len = 0\n",
    "  input_batch = []\n",
    "  for i in range(0,batch_size):\n",
    "    batch = dataset[0][idx[i]]\n",
    "    if len(batch) > max_inp_len:\n",
    "      max_inp_len = len(batch)\n",
    "\n",
    "  # 补足长度\n",
    "  for i in range(0,batch_size):\n",
    "    batch = dataset[0][idx[i]]\n",
    "    batch = batch + [0]*(max_inp_len - len(batch))\n",
    "    input_batch += [batch]\n",
    " \n",
    "  '''construct a list of output sequences for the training batch'''\n",
    "  max_output_len = 0\n",
    "  output_batch = []\n",
    "  for i in range(0,batch_size):\n",
    "    batch = dataset[1][idx[i]]\n",
    "    if len(batch) > max_output_len:\n",
    "      max_output_len = len(batch)\n",
    "\n",
    "  for i in range(0,batch_size):\n",
    "    batch = dataset[1][idx[i]]\n",
    "    batch = batch + [0]*(max_output_len - len(batch))\n",
    "    output_batch += [batch]    \n",
    "\n",
    "  # x_batch, y_batch provide the true inputs and targets for network training\n",
    "  x_batch = np.reshape(input_batch, [batch_size, max_inp_len])\n",
    "  y_batch = np.reshape(output_batch, [batch_size, max_output_len])\n",
    "  return x_batch, y_batch\n",
    "\n",
    "# Perform some simple tests to make sure your batch function is working properly! \n",
    "a,b = get_batch(dataset,  2)\n",
    "print(\"{} -> {}\".format(a[0],b[0]))\n",
    "print(\"{} -> {}\".format(a[1],b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## 位置编码\n",
    "\n",
    "## Positional encoding\n",
    "\n",
    "因为transformer模型是没有包含顺序信息的，所以增加位置编码给模型提供每个字的位置相关的信息。\n",
    "\n",
    "位置编码向量可以直接加到文字的embedding向量上。位置信息编码计算方式如下：\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## 掩码\n",
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "喂数据的时候，为了并行计算方便，是按固定长度输入数据的，但是实际数据有长有短，所以把数据填充上一个特殊的 pad token，保证每个喂给模型的数据都是固定长度的。\n",
    "而Masking，就是遮挡住序列里所有 pad token。避免模型把 pad 当成输入。mask实际上是一个数组，输入有值的位置是0，没有值的位置是1。\n",
    "\n",
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "另一种是 look-ahead mask，用来遮挡住序列里未来的（还没算出来的）数据。意味着，当预测第三个字的时候，只有第一、第二个字可以被用到。\n",
    "\n",
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## 点乘 attention\n",
    "\n",
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "transformer的基础，就是attention。\n",
    "简要的说，通过 q,k,v 三个向量参数，计算出每个输入对应的输出结果，其中，q用来匹配其他所有的k，v是基础值。\n",
    "q,k,v是用来训练的主要参数。\n",
    "\n",
    "下面给出了一些例子，当q和k不同的时候，注意attention计算结果是如何变化的。\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "dff是feed forward层输出向量的纬度，d_model是模型输出向量\n",
    "\n",
    "\n",
    "纬度。\n",
    "\n",
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mytb1lPyOHLB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 64))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzZRXdO0mI48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QG9nueFQKXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8500])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    vocab_size=8500, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "MAX_LENGTH = 65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          vocab_size+2,\n",
    "                          pe_input=vocab_size+2, \n",
    "                          pe_target=vocab_size+2,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try restore...\n",
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./couplet_checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  print(\"try restore...\")\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peeking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbvmaKNiznHZ",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10000\n",
    "\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "history = []\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  #for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "  inp, tar = get_batch(dataset, TRAIN_BATCH_SIZE)\n",
    "  train_step(inp, tar)\n",
    "    \n",
    "  history.append(train_loss.result())\n",
    "  if (epoch + 1) % 50 == 0:\n",
    "    plotter.plot(history)\n",
    "\n",
    "  if (epoch + 1) % 2000 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "#    print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "#    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n",
    "#    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    " \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = [vocab_size] + vectorize_string(inp_sentence.replace(\"\\n\",\"\")) + [(vocab_size+1)]\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "#     predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "    predictions /= 1.0\n",
    "    predicted_id = tf.cast(tf.random.categorical(predictions[0], num_samples=1), tf.int32)\n",
    "\n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, np.asarray(predicted_id)], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = \"\"\n",
    "  for i in result:\n",
    "    if i < vocab_size:\n",
    "      predicted_sentence += idx2char[i]\n",
    "\n",
    "  print('上联: {}'.format(sentence))\n",
    "  print('下联: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsxrAlvFG8SZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上联: 殷勤怕负三春意\n",
      "下联: 淡泊难离万水愁\n",
      "上联: 如此清秋何吝酒\n",
      "下联: 这边明月不欺天\n",
      "上联: 天朗气清风和畅\n",
      "下联: 品明影碧自澄纯\n",
      "上联: 梦里不知身是客\n",
      "下联: 年中未助兴逐浪\n",
      "上联: 千秋月色君长看\n",
      "下联: 万里云云我暗迎\n",
      "上联: 天若有情天亦老\n",
      "下联: 紫气生根紫临华\n",
      "上联: 冰比冰水冰\n",
      "下联: 法因法多法\n",
      "上联: 上海自来水来自海上\n",
      "下联: 红云游舞胡舞游云红\n",
      "上联: 老汉推车\n",
      "下联: 公仆绘图\n"
     ]
    }
   ],
   "source": [
    "translate(\"殷勤怕负三春意\")\n",
    "translate(\"如此清秋何吝酒\")\n",
    "translate(\"天朗气清风和畅\")\n",
    "translate(\"梦里不知身是客\")\n",
    "translate(\"千秋月色君长看\")\n",
    "translate(\"天若有情天亦老\")\n",
    "translate(\"冰比冰水冰\")\n",
    "translate(\"上海自来水来自海上\")\n",
    "translate(\"老汉推车\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "couplet.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb",
     "timestamp": 1590736463811
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
